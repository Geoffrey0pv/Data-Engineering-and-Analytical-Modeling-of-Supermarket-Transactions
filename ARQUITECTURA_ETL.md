# ğŸ—ï¸ Arquitectura ETL Completa - Sistema de AnÃ¡lisis de Supermercado

## ğŸ“‹ Ãndice
1. [Vista General de la Infraestructura](#vista-general)
2. [Componentes Docker](#componentes-docker)
3. [Flujo de Datos Completo](#flujo-de-datos)
4. [Procesamiento Distribuido](#procesamiento-distribuido)
5. [Limitaciones y Por QuÃ© Ocurren](#limitaciones)
6. [Soluciones Industriales](#soluciones-industriales)

---

## ğŸŒ Vista General de la Infraestructura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ARQUITECTURA ETL                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   AIRFLOW    â”‚â”€â”€â”€â”€â–¶â”‚     SPARK    â”‚â”€â”€â”€â”€â–¶â”‚   OUTPUTS    â”‚   â”‚
â”‚  â”‚  Scheduler   â”‚     â”‚   Cluster    â”‚     â”‚  (Parquet/   â”‚   â”‚
â”‚  â”‚  Webserver   â”‚     â”‚  Master+     â”‚     â”‚    CSV)      â”‚   â”‚
â”‚  â”‚              â”‚     â”‚  Workers     â”‚     â”‚              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                     â”‚                     â”‚           â”‚
â”‚         â”‚                     â”‚                     â”‚           â”‚
â”‚         â–¼                     â–¼                     â–¼           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              VOLÃšMENES COMPARTIDOS (Docker)               â”‚ â”‚
â”‚  â”‚  /data/raw  â”‚  /data/processed  â”‚  /data/analytics      â”‚ â”‚
â”‚  â”‚  /scripts   â”‚  /logs             â”‚  /dags                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ³ Componentes Docker

### **1. Airflow (Orquestador)**

**Contenedor:** `airflow-webserver` + `airflow-scheduler`

```yaml
# docker-compose.yml
airflow-webserver:
  image: apache/airflow:2.x
  ports:
    - "8080:8080"  # UI Web
  volumes:
    - ./dags:/opt/airflow/dags           # DAGs Python
    - ./data:/opt/airflow/data           # Datos compartidos
    - ./scripts:/opt/spark/scripts       # Scripts PySpark
    - ./logs:/opt/airflow/logs           # Logs de ejecuciÃ³n
```

**Responsabilidades:**
- âœ… Programar tareas (scheduling)
- âœ… Monitorear estado del pipeline
- âœ… Reintentar tareas fallidas
- âœ… Gestionar dependencias entre tareas
- âš ï¸ **NO procesa datos** (solo orquesta)

**Recursos Asignados:**
```
CPU: Compartido (sin lÃ­mite)
RAM: Compartido (sin lÃ­mite)
Almacenamiento: VolÃºmenes Docker
```

---

### **2. Spark Master (Coordinador)**

**Contenedor:** `spark-master`

```yaml
spark-master:
  image: bitnami/spark:3.5
  ports:
    - "7077:7077"  # Puerto de comunicaciÃ³n con workers
    - "8081:8080"  # UI Web Spark Master
  environment:
    - SPARK_MODE=master
```

**Responsabilidades:**
- âœ… Recibir trabajos de Airflow
- âœ… Dividir tareas en stages
- âœ… Asignar tareas a workers
- âœ… Gestionar shuffle operations
- âš ï¸ **NO ejecuta cÃ³digo** (solo coordina)

**Recursos Asignados:**
```
CPU: ~1-2 cores
RAM: ~512 MB - 1 GB
Red: Puerto 7077 (RPC)
```

---

### **3. Spark Workers (Ejecutores)**

**Contenedores:** `spark-worker-1`, `spark-worker-2`, ...

```yaml
spark-worker-1:
  image: bitnami/spark:3.5
  environment:
    - SPARK_MODE=worker
    - SPARK_MASTER_URL=spark://spark-master:7077
    - SPARK_WORKER_MEMORY=1G      # 1 GB de RAM por worker
    - SPARK_WORKER_CORES=2        # 2 CPU cores por worker
```

**Responsabilidades:**
- âœ… **EJECUTAR CÃ“DIGO PYSPARK** (transformaciones)
- âœ… Leer/escribir datos del filesystem
- âœ… Cachear RDDs en memoria
- âœ… Realizar operaciones shuffle
- âœ… Reportar estado al Master

**Recursos Asignados (por worker):**
```
CPU: 2 cores
RAM: 1 GB
Almacenamiento: /tmp (temporal para shuffle)
```

---

## ğŸ”„ Flujo de Datos Completo (ETL Pipeline)

### **Paso 1: Inicio del DAG (Airflow)**

```python
# dags/supermarket_etl_dag.py
@dag(
    schedule_interval='@daily',  # Ejecuta diariamente
    start_date=datetime(2025, 1, 1),
    catchup=False
)
def supermarket_etl_pipeline():
    # ...
```

**Â¿QuÃ© pasa aquÃ­?**
1. **Airflow Scheduler** escanea el directorio `/dags` cada 5 segundos
2. Detecta que el DAG debe ejecutarse
3. Crea un **DagRun** (instancia de ejecuciÃ³n)
4. Encola las tareas en la cola de ejecuciÃ³n

**UbicaciÃ³n fÃ­sica:**
```
Container: airflow-scheduler
Proceso: Python (airflow/jobs/scheduler_job.py)
CPU: MÃ­nimo (~100 MB RAM, 0.1 CPU)
```

---

### **Paso 2: FileSensor (Espera de Datos)**

```python
wait_for_data = FileSensor(
    task_id='wait_for_raw_data',
    filepath='/opt/airflow/data/raw/Transactions/*.csv',
    fs_conn_id='fs_default',
    poke_interval=30  # Revisa cada 30 segundos
)
```

**Â¿QuÃ© pasa aquÃ­?**
1. Airflow Worker ejecuta un **poller** que revisa si existen archivos
2. Si NO existen â†’ Espera 30s y vuelve a revisar
3. Si existen â†’ Marca la tarea como SUCCESS
4. **NO MUEVE DATOS**, solo verifica existencia

**UbicaciÃ³n fÃ­sica:**
```
Container: airflow-webserver (executor local)
Filesystem: /opt/airflow/data/raw/Transactions/
OperaciÃ³n: os.path.exists() - sin carga de CPU
```

---

### **Paso 3: TransformaciÃ³n PySpark (spark_transform)**

```python
spark_transform = SparkSubmitOperator(
    task_id='spark_transform',
    application='/opt/spark/scripts/transform_data.py',
    conn_id='spark_default',  # spark://spark-master:7077
    conf={
        'spark.executor.memory': '1g',
        'spark.executor.cores': '2'
    }
)
```

**Â¿QuÃ© pasa aquÃ­? (Paso a Paso)**

#### **3.1. Airflow envÃ­a el job a Spark Master**
```
Airflow Webserver 
    â†“ (HTTP REST API)
Spark Master (puerto 7077)
    â†“ "Necesito ejecutar /opt/spark/scripts/transform_data.py"
```

#### **3.2. Spark Master crea un DAG de ejecuciÃ³n**
```python
# Spark internamente hace:
# 1. Leer CSV â†’ RDD[Row]
df = spark.read.csv("/opt/airflow/data/raw/Transactions/*.csv")

# Esto se convierte en:
# Stage 0: Scan CSV files (narrow transformation)
# â””â”€ Task 0: Read partition 1
# â””â”€ Task 1: Read partition 2
# â””â”€ Task 2: Read partition 3
```

#### **3.3. Spark Master asigna tareas a Workers**
```
Spark Master:
  "Worker-1: Ejecuta Task 0 (partition 1)"
  "Worker-2: Ejecuta Task 1 (partition 2)"

Worker-1 (Container spark-worker-1):
  â”œâ”€ Lee /data/raw/Transactions/part-00001.csv
  â”œâ”€ Parsea CSV â†’ RDD[Row]
  â””â”€ Mantiene en memoria (cache)
```

#### **3.4. Transformaciones (filtros, joins, agregaciones)**
```python
# Script: transform_data.py
df_transactions.join(df_products, "ID_Producto") \
    .filter(col("Precio") > 0) \
    .groupBy("ID_Tienda") \
    .agg(sum("Precio"))

# Esto genera:
# Stage 1: Join (SHUFFLE) â† Mueve datos entre workers
#   â”œâ”€ Worker-1 envÃ­a particiones a Worker-2
#   â””â”€ Worker-2 envÃ­a particiones a Worker-1
#
# Stage 2: Aggregation (SHUFFLE)
#   â””â”€ Agrupa por ID_Tienda (puede requerir mover datos)
```

**ğŸ”´ AQUÃ OCURRE EL CUELLO DE BOTELLA:**
```
Worker-1: "Necesito datos de la particiÃ³n 5"
    â†“ (Red TCP/IP entre containers)
Worker-2: "AquÃ­ estÃ¡n los 50 MB de datos"
    â†“ (Si Worker-2 se cae o se queda sin RAM...)
Worker-1: "ERROR: Missing shuffle partition 5"
    â†“
Job FAILS (MetadataFetchFailedException)
```

#### **3.5. Escritura de Resultados**
```python
df_master.write \
    .mode("overwrite") \
    .parquet("/opt/airflow/data/processed/transactions_master_latest")

# Spark hace:
# Stage 3: Write to disk
#   â”œâ”€ Worker-1 escribe part-00000.parquet
#   â”œâ”€ Worker-2 escribe part-00001.parquet
#   â””â”€ Worker-3 escribe part-00002.parquet
```

**UbicaciÃ³n fÃ­sica:**
```
Containers: spark-worker-1, spark-worker-2
Procesos: JVM (Executor) ejecutando PySpark
CPU: 2 cores Ã— 2 workers = 4 cores total
RAM: 1 GB Ã— 2 workers = 2 GB total
Network: TCP entre workers (shuffle)
Disk I/O: Lectura de CSV + Escritura de Parquet
```

---

### **Paso 4: AnÃ¡lisis Avanzado (FP-Growth)**

```python
spark_advanced_analytics = SparkSubmitOperator(
    task_id='spark_advanced_analytics',
    application='/opt/spark/scripts/train_models.py'
)
```

**Â¿QuÃ© pasa aquÃ­? (Por QuÃ© Falla)**

#### **4.1. ConstrucciÃ³n del FP-Tree**
```python
# train_models.py
df_transactions = df_master.groupBy("ID_Transaccion").agg(
    collect_set("ID_Producto").alias("items")
)

# Spark hace:
# Stage N: Group by transaction â†’ SHUFFLE
#   â”œâ”€ Todas las transacciones con mismo ID 
#   â”‚   deben ir al mismo worker
#   â””â”€ Si hay 1M de transacciones Ãºnicas...
#       â””â”€ 1M de particiones a redistribuir!
```

**ğŸ”´ PROBLEMA 1: Shuffle Masivo**
```
Worker-1 tiene: [Tx1, Tx2, Tx3, Tx5, Tx7, ...]
Worker-2 necesita: [Tx1, Tx3, Tx5] para su partition

Worker-1 envÃ­a 500 MB de datos â†’ Worker-2
Worker-2 envÃ­a 450 MB de datos â†’ Worker-1
...
Total datos movidos: 2-3 GB entre workers

Si Worker-2 se queda sin RAM mientras recibe:
  â†’ OutOfMemoryError
  â†’ Executor KILLED (exit code 52)
  â†’ Shuffle metadata LOST
  â†’ Job FAILS
```

#### **4.2. Algoritmo FP-Growth**
```python
model = FPGrowth(minSupport=0.10).fit(df_transactions)

# Spark hace internamente:
# Stage N+1: Contar frecuencia de items â†’ SHUFFLE
# Stage N+2: Ordenar por frecuencia â†’ SHUFFLE
# Stage N+3: Construir FP-Tree en memoria â†’ RAM INTENSIVO
#   â”œâ”€ Si hay 10,000 productos Ãºnicos
#   â”‚   y 100,000 transacciones...
#   â””â”€ FP-Tree puede ocupar 500 MB - 2 GB de RAM
#
# Stage N+4: Minar patrones â†’ SHUFFLE
#   â”œâ”€ Para cada nodo del Ã¡rbol
#   â”‚   genera combinaciones recursivas
#   â””â”€ Millones de combinaciones â†’ SHUFFLE masivo
#
# Stage N+5: Generar reglas â†’ SHUFFLE
```

**ğŸ”´ PROBLEMA 2: Memoria Insuficiente**
```
Worker-1 (1 GB RAM):
  â”œâ”€ JVM overhead: 200 MB
  â”œâ”€ PySpark process: 100 MB
  â”œâ”€ Cached RDDs: 300 MB
  â”œâ”€ FP-Tree construction: 600 MB
  â””â”€ TOTAL: 1.2 GB > 1 GB limit
      â†’ OutOfMemoryError
      â†’ Job FAILS
```

**UbicaciÃ³n fÃ­sica:**
```
Containers: spark-worker-1, spark-worker-2
RAM Utilizada: 90-100% (1 GB cada uno)
CPU: 80-100% (procesamiento intensivo)
Network: Saturada (shuffle operations)
Disk: /tmp lleno (spill to disk cuando RAM llena)
```

---

## âš™ï¸ Procesamiento Distribuido: Â¿CÃ³mo Funciona Realmente?

### **Conceptos Clave:**

#### **1. Particionamiento de Datos**
```python
# Cuando lees 1M de registros con 2 workers:
df = spark.read.csv("data.csv")  # 1M rows

# Spark divide automÃ¡ticamente:
Worker-1: Partitions [0-99]    â†’ 500K rows
Worker-2: Partitions [100-199] â†’ 500K rows

# Cada worker procesa SU particiÃ³n independientemente
```

#### **2. Transformaciones Narrow vs Wide**

**Narrow (sin shuffle - RÃPIDO):**
```python
df.filter(col("price") > 10)   # Cada worker filtra su data
df.select("name", "price")     # Cada worker selecciona columnas
df.withColumn("tax", col("price") * 0.15)  # CÃ¡lculo local
```

**Wide (con shuffle - LENTO):**
```python
df.groupBy("category").count()  # Requiere mover datos entre workers
df.join(other_df, "id")         # Requiere redistribuir ambos DFs
df.distinct()                   # Requiere comparar todas las particiones
```

#### **3. Shuffle: El Cuello de Botella**
```
ANTES del shuffle:
Worker-1: [Category A: 100, Category B: 50]
Worker-2: [Category A: 80, Category B: 120]

SHUFFLE (redistribuir por key):
Worker-1 envÃ­a [Category B: 50] â†’ Worker-2
Worker-2 envÃ­a [Category A: 80] â†’ Worker-1

DESPUÃ‰S del shuffle:
Worker-1: [Category A: 180]  â† Suma 100 + 80
Worker-2: [Category B: 170]  â† Suma 50 + 120

ğŸ”´ Si Worker-2 muere durante el envÃ­o:
  â†’ Worker-1 no recibe [Category A: 80]
  â†’ MetadataFetchFailedException
```

---

## ğŸš¨ Limitaciones de Tu Infraestructura Actual

### **1. Recursos Insuficientes para FP-Growth**

```
TU CLUSTER:
â”œâ”€ Worker-1: 1 GB RAM, 2 cores
â”œâ”€ Worker-2: 1 GB RAM, 2 cores
â””â”€ TOTAL:    2 GB RAM, 4 cores

FP-GROWTH con 1M transacciones necesita:
â”œâ”€ Shuffle data: ~5-10 GB
â”œâ”€ FP-Tree RAM: ~2-4 GB
â””â”€ Intermediate results: ~2-3 GB
    TOTAL: ~10-15 GB RAM

RESULTADO: 2 GB << 15 GB â†’ CRASH inevitable
```

### **2. Red Interna Docker (Latencia)**
```
Shuffle entre workers:
Container A â†’ Docker Network â†’ Container B

Latencia tÃ­pica: 1-5 ms
Con millones de operaciones: 1-5 horas de espera solo en red
```

### **3. Disco /tmp Limitado**
```
Cuando RAM se llena, Spark "spills" a disco:
/tmp/spark-shuffle-xxx

Disco container: ~10 GB
Shuffle data: ~20 GB
â†’ Disk full â†’ Job FAILS
```

---

## ğŸ­ CÃ³mo se Hace en la Industria

### **Caso Real: Amazon**

```
INFRAESTRUCTURA:
â”œâ”€ 500+ workers en AWS EMR (Elastic MapReduce)
â”œâ”€ Cada worker: 32 GB RAM, 8 cores
â”œâ”€ Red de 10 Gbps entre workers
â””â”€ S3 para almacenamiento (ilimitado)

OPTIMIZACIONES:
1. Pre-filtrado: Solo Top 1000 productos
2. SegmentaciÃ³n: FP-Growth por categorÃ­a (no global)
3. Sampling: 1-5% de transacciones (suficiente para patrones)
4. Caching: Redis para resultados frecuentes
5. Incremental: Solo procesar transacciones nuevas
```

### **Caso Real: Netflix**

```
NO USAN FP-GROWTH para recomendaciones!

Usan:
1. Collaborative Filtering (ALS - Alternating Least Squares)
   - MÃ¡s escalable
   - Menos shuffle
   - Resultados mejores

2. Deep Learning (Transformers)
   - Modelos pre-entrenados
   - Fine-tuning con batch pequeÃ±os

3. Graph Databases (Neo4j)
   - Relaciones user-item como grafo
   - Queries rÃ¡pidas sin shuffle
```

---

## âœ… Soluciones para Tu Proyecto

### **OpciÃ³n 1: Simplificar (Recomendado para 1M rows)**
```python
# NO hagas FP-Growth completo
# Haz anÃ¡lisis de co-ocurrencia simple:

df_cooccurrence = df_transactions \
    .groupBy("product_a", "product_b") \
    .count() \
    .filter(col("count") > 100) \
    .orderBy("count", descending=True) \
    .limit(100)

# Mucho mÃ¡s rÃ¡pido, mismos insights
```

### **OpciÃ³n 2: Aumentar Recursos**
```yaml
# docker-compose.yml
spark-worker-1:
  deploy:
    resources:
      limits:
        memory: 4G  # Aumentar a 4 GB
        cpus: '4'   # Aumentar a 4 cores
```

### **OpciÃ³n 3: Segmentar + Procesar por Partes**
```python
# train_models.py (modificado)
for category in ["Electronics", "Grocery", "Fashion"]:
    df_category = df_master.filter(col("category") == category)
    
    # FP-Growth solo en esta categorÃ­a (mÃ¡s manejable)
    model = FPGrowth(minSupport=0.10).fit(df_category)
    model.save(f"models/fpgrowth_{category}")
```

---

## ğŸ“Š Diagrama de Secuencia Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Usuarioâ”‚    â”‚ Airflow â”‚    â”‚   Spark   â”‚    â”‚  Spark   â”‚    â”‚  Datos  â”‚
â”‚  Web   â”‚    â”‚ Schedulerâ”‚   â”‚   Master  â”‚    â”‚  Workers â”‚    â”‚ (Disk)  â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚              â”‚                â”‚                â”‚               â”‚
    â”‚ 1. Trigger  â”‚                â”‚                â”‚               â”‚
    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                â”‚                â”‚               â”‚
    â”‚  DAG Run    â”‚                â”‚                â”‚               â”‚
    â”‚              â”‚                â”‚                â”‚               â”‚
    â”‚              â”‚ 2. Submit Job â”‚                â”‚               â”‚
    â”‚              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚                â”‚               â”‚
    â”‚              â”‚                â”‚                â”‚               â”‚
    â”‚              â”‚                â”‚ 3. Assign Tasksâ”‚               â”‚
    â”‚              â”‚                â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚               â”‚
    â”‚              â”‚                â”‚                â”‚               â”‚
    â”‚              â”‚                â”‚                â”‚ 4. Read Data  â”‚
    â”‚              â”‚                â”‚                â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
    â”‚              â”‚                â”‚                â”‚               â”‚
    â”‚              â”‚                â”‚                â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
    â”‚              â”‚                â”‚                â”‚  CSV/Parquet  â”‚
    â”‚              â”‚                â”‚                â”‚               â”‚
    â”‚              â”‚                â”‚ 5. SHUFFLE     â”‚               â”‚
    â”‚              â”‚                â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚               â”‚
    â”‚              â”‚                â”‚  (Data Xfer)   â”‚               â”‚
    â”‚              â”‚                â”‚                â”‚               â”‚
    â”‚              â”‚                â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚               â”‚
    â”‚              â”‚                â”‚ 6. Results     â”‚               â”‚
    â”‚              â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚               â”‚
    â”‚              â”‚   Job Complete â”‚                â”‚               â”‚
    â”‚<â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                â”‚                â”‚               â”‚
    â”‚ 7. Status OK â”‚                â”‚                â”‚               â”‚
    â”‚              â”‚                â”‚                â”‚               â”‚
```

---

## ğŸ“ˆ MÃ©tricas de Tu Sistema Actual

```
CAPACIDAD TEÃ“RICA:
â”œâ”€ Throughput: ~10-50 MB/s por worker
â”œâ”€ Max rows/sec: ~10,000 - 50,000
â””â”€ Processing time: 1M rows en 20-100 segundos (sin shuffle)

CON FP-GROWTH (shuffle intensivo):
â”œâ”€ Throughput: ~1-5 MB/s (20x mÃ¡s lento)
â”œâ”€ Max rows: ~50,000 - 100,000 (sin crash)
â””â”€ Processing time: 1M rows â†’ OutOfMemory o 30-60 minutos

RECOMENDACIÃ“N:
Para 1M rows con FP-Growth:
  Minimum: 8 GB RAM total (4 workers Ã— 2 GB)
  Optimal: 16 GB RAM total (4 workers Ã— 4 GB)
```

---

## ğŸ¯ ConclusiÃ³n

**Tu infraestructura ESTÃ funcionando correctamente:**
- âœ… Airflow orquesta bien
- âœ… Spark distribuye tareas correctamente
- âœ… Workers ejecutan cÃ³digo en paralelo

**El problema NO es la infraestructura, es el algoritmo:**
- âŒ FP-Growth es inherentemente costoso
- âŒ 1M transacciones es demasiado para 2 GB RAM
- âŒ Shuffle operations saturan la red y memoria

**Soluciones profesionales:**
1. âœ… Sampling (10-20% de datos)
2. âœ… Pre-filtrado (Top-N productos)
3. âœ… SegmentaciÃ³n (por categorÃ­a/regiÃ³n)
4. âœ… Algoritmos alternativos (ALS, Graph-based)
5. âœ… Aumentar recursos (4-8 GB por worker)

**Para tu caso (1M rows):**
- Usa sampling al 10% + minSupport=0.10 (actual)
- O usa co-ocurrencia simple en lugar de FP-Growth
- O aumenta RAM a 4 GB por worker si es posible

---

## ğŸ“š Referencias

- [Apache Spark Tuning Guide](https://spark.apache.org/docs/latest/tuning.html)
- [FP-Growth Paper](https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf)
- [Amazon Recommendations Architecture](https://www.amazon.science/publications)
- [Netflix Recommendations](https://netflixtechblog.com/netflix-recommendations-beyond-the-5-stars-part-1-55838468f429)
